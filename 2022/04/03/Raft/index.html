

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" href="/img/favicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Leo Shen">
  <meta name="keywords" content="">
  
    <meta name="description" content="在MIT6.824的课程中，Raft算法在Lab中是一个举足轻重的存在，除了MapReduce，其余三个Lab都基于Raft模块的实现。本文将主要介绍Raft算法。">
<meta property="og:type" content="article">
<meta property="og:title" content="MIT6.824 -&gt; Raft">
<meta property="og:url" content="http://yoursite.com/2022/04/03/Raft/index.html">
<meta property="og:site_name" content="Leo的博客">
<meta property="og:description" content="在MIT6.824的课程中，Raft算法在Lab中是一个举足轻重的存在，除了MapReduce，其余三个Lab都基于Raft模块的实现。本文将主要介绍Raft算法。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://yoursite.com/picture/Distributed/surface.png">
<meta property="article:published_time" content="2022-04-03T13:03:00.000Z">
<meta property="article:modified_time" content="2022-04-05T05:01:02.755Z">
<meta property="article:author" content="Leo Shen">
<meta property="article:tag" content="分布式">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://yoursite.com/picture/Distributed/surface.png">
  
  
  <title>MIT6.824 -&gt; Raft - Leo的博客</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css" />
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hint.css@2/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css" />
  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","version":"1.8.14","typing":{"enable":true,"typeSpeed":60,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":true}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.4.1"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Leo Shen</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/img/default.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="MIT6.824 -&gt; Raft">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2022-04-03 21:03" pubdate>
        2022年4月3日 晚上
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      14k 字
    </span>
  

  
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      115 分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">MIT6.824 -&gt; Raft</h1>
            
              <p class="note note-info">
                
                  本文最后更新于：19 天前
                
              </p>
            
            <div class="markdown-body">
              <p>在MIT6.824的课程中，Raft算法在Lab中是一个举足轻重的存在，除了MapReduce，其余三个Lab都基于Raft模块的实现。本文将主要介绍Raft算法。</p>
<span id="more"></span>

<p>MIT6.824课程先从Google三驾马车MapReduce、GFS、Bigtable中的两个开始，分别引入分布式场景下的计算与存储问题。对于分布式计算，这是一个无状态的过程，从MapReduce到Spark对其进行各种改进（如一个节点可以同时执行多种Map任务）；对于分布式存储，这个话题更加复杂，状态的维护与对外的展现是一个极其棘手的问题，<strong>GFS</strong>为了追求性能并不能保证多副本的数据一致性，若Secondary chunk server出现错误则已经写入的节点是无法被Primary chunk server纠正的（当然也主要因为业务需求是Web搜索，精确性要求没那么高），而本文的主角<strong>Raft</strong>则是一个能够维护多副本间一致的一个共识算法，并能够轻易对外界服务提供线性一致性的一致性模型保证。</p>
<h3 id="Raft-的由来与宗旨"><a href="#Raft-的由来与宗旨" class="headerlink" title="Raft 的由来与宗旨"></a>Raft 的由来与宗旨</h3><p>众所周知，Paxos 是一个非常划时代的共识算法。在 Raft 出现之前的 10 年里，Paxos 几乎统治着共识算法这一领域：因为绝大多数共识算法的实现都是基于 Paxos 或者受其影响，同时 Paxos 也成为了教学领域里讲解共识问题时的示例。</p>
<p>但是不幸的是，尽管有很多工作都在尝试降低 Paxos 的复杂性，但是它依然十分难以理解。并且，Paxos 自身的算法结构需要进行大幅的修改才能够应用到实际的系统中。这些都导致了工业界和学术界都对 Paxos 算法感到十分头疼。比如 <code>Google Chubby</code> 的论文就提到，因为 Paxos 的描述和现实差距太大，所以最终人们总会实现一套未经证实的类 Paxos 协议。</p>
<p>基于以上背景，<code>Diego Ongaro</code> 在就读博士期间，深入研究 Paxos 协议后提出了 Raft 协议，旨在提供更为易于理解的共识算法。Raft 的宗旨在于可实践性和可理解性，并且相比 Paxos 几乎没有牺牲多少性能。</p>
<h3 id="Raft简介"><a href="#Raft简介" class="headerlink" title="Raft简介"></a>Raft简介</h3><p>共识算法的目标就是保证集群上所有节点的状态一致。那么关于这样的副本一致问题主要有两种思路：</p>
<p>①<strong>状态转移</strong>即完全拷贝节点状态（一般在新节点刚加入集群上时采用此方案）</p>
<p>②<strong>复制状态机</strong>即仅将来自客户端的操作或其他外部事件从Primary节点传输到其他节点（这也是大多数情况采用的方案）</p>
<p><img src="/picture/Distributed/state_machine.png" srcset="/img/loading.gif" lazyload alt="state machine"></p>
<p>对于节点一般有两种操作即读操作与写操作。写操作会改变节点状态，便立刻要<strong>复制状态机</strong>到其他节点上（将写指令同步给其他所有节点）。理想状态下我们希望这种同步可以立即作用于其他节点之上，但由于<strong>不可靠的网络</strong>将出现<strong>分区、冗余、丢失、乱序</strong>等问题。如何保证指令能够在所有节点上都以相同的顺序执行呢？这便是Raft所要解决的问题。</p>
<h4 id="一个简易的可行方案"><a href="#一个简易的可行方案" class="headerlink" title="一个简易的可行方案"></a>一个简易的可行方案</h4><ol>
<li><p>首先在多个副本中选出一个Leader节点负责发送同步日志到其他节点，并确定日志顺序。</p>
</li>
<li><p>所有需要写日志的请求（在未做ReadIndex等优化时默认读请求也要写日志）均需发送给Leader节点。</p>
</li>
<li><p>Leader先将日志写入自己的磁盘，日志需要通过某个参数（Index）定义顺序，并发送给其他节点，超过半数的节点同意了此操作Leader节点便会回复服务调用者真正apply该操作。</p>
</li>
<li><p>当Leader节点崩溃时，其他跟随者节点应通过HeartBeat机制重新选举出新的Leader保证集群的正常运行。</p>
</li>
<li><p>当集群新加入节点或有节点退出时时，需要将配置信息同步到集群内所有节点。</p>
</li>
</ol>
<h3 id="Raft的详细实现"><a href="#Raft的详细实现" class="headerlink" title="Raft的详细实现"></a>Raft的详细实现</h3><p>根据上文提供的一个简易的可行方案，可以将共识算法这个很难解决的问题分为多个子问题：</p>
<ul>
<li><p>1：<code>Leader election </code>选举领导节点统筹全局。</p>
</li>
<li><p>2 - 3：<code>Log Replication</code>日志同步 Leader负责从客户端接收请求，并且在集群中扩散同步。</p>
</li>
<li><p>4：<code>Safety</code> 各节点间状态机的一致性保证。</p>
</li>
<li><p>… …</p>
</li>
</ul>
<blockquote>
<p>这里简单提一下为什么到此说的所有操作都是以日志形式来记录。Log的作用非常非常大，主要有以下几点：</p>
<ol>
<li>方便定义顺序（Index）</li>
<li>Follower通过Log临时存取操作从而能够等待Commit消息</li>
<li>Leader也需要存储Log来对Follower进行重传保证消息不丢失</li>
<li>Log可以在节点重启时帮助恢复状态（Follower需要等待Leader的指示）</li>
<li>通过Log携带后续将介绍的任期号Term，可以确定日志新旧 / 是否可以被提交等问题（解决了GFS已写入的错误日志无法被纠正的问题）</li>
</ol>
<p>总之使用日志机制能够帮助解决非常多的问题，文件系统、数据库也都利用了日志帮助解决可靠的持久化（崩溃后的一致性）、原子性等问题。</p>
<p><strong>Log = Index + Term + cmd</strong></p>
</blockquote>
<h4 id="节点的状态机"><a href="#节点的状态机" class="headerlink" title="节点的状态机"></a>节点的状态机</h4><p>既然Raft节点有Leader状态，自然也有其他Follower状态，节点间状态的变化较为重要。Raft将节点分为<code>Leader</code>、<code>Candidate</code>、<code>Follower</code>三种状态。现在看可能有些不明白，可以等第二遍看就清晰多了。简单来说就是集群内会有一个Leader负责发起心跳，响应客户端，创建日志，同步日志。其余节点都是Follower，但每个节点内部都会有个超时计时器用于检测自己是否还能和Leader互联（当接受到Leader心跳或同步日志请求便会重置该计时器），如果超时Follower便会将自己转变为候选者Candidate向所有节点索要投票（Term任期号也加一），一旦超过半数的节点同意便会成为当选Leader（后续会什么条件下节点会同意投票），否则可能会再次等待超时器超时重新参选或被其他Leader的消息（含有更新的任期号）打回Follower。</p>
<blockquote>
<p>Term任期号保证了安全性，一个任期每个节点只有一张票即能保证半数以上票真的是半数以上的节点投出的，这样便能保证对于一个给定任期号最多只有一个领导者。</p>
<p>Term 在 Raft 算法中充当逻辑时钟。可以作为感受节点状态是否过期的标志。</p>
</blockquote>
<p>  <img src="/picture/Distributed/state_transfer.png" srcset="/img/loading.gif" lazyload alt="state transfer"></p>
<p>在博士论文和实际生产系统中，其实又增加了两种身份：</p>
<ul>
<li><code>Learner</code>：不具有选举权，参与日志复制过程但不计数的节点。可以作为新节点加入集群时的过渡状态以提升可用性，也可以作为一种类似于 binlog 的对 Leader 日志流进行订阅的角色，比如可以参考 PingCAP 公司 tikv 和 tiflash 的架构。</li>
<li><code>Pre candidate</code>：刚刚发起竞选，还在等待 <code>Pre-Vote</code> 结果的临时状态， 取决于 <code>Pre-Vote</code> 的结果，可能进化为 candidate，可能退化为 follower。</li>
</ul>
<h4 id="节点状态的数据结构"><a href="#节点状态的数据结构" class="headerlink" title="节点状态的数据结构"></a>节点状态的数据结构</h4><p>每一个节点都应该有的持久化状态：</p>
<ul>
<li><code>currentTerm</code>：当前任期，保证重启后任期不丢失。</li>
<li><code>votedFor</code>：在当前 term，给哪个节点投了票，值为 null 或 <code>candidate id</code>。即使节点重启，Raft 算法也能保证每个任期最多只有一个 leader。</li>
<li><code>log[]</code>：已经 committed 的日志，保证状态机可恢复。</li>
</ul>
<p>每一个节点都应该有的非持久化状态：</p>
<ul>
<li><code>commitIndex</code>：已提交的最大 index。leader 节点重启后可以通过 appendEntries RPC 逐渐得到不同节点的 matchIndex，从而确认 commitIndex，follower 只需等待 leader 传递过来的 commitIndex 即可。</li>
<li><code>lastApplied</code>：已被状态机应用的最大 index。raft 算法假设了状态机本身是易失的，所以重启后状态机的状态可以通过 log[] （部分 log 可以压缩为 snapshot) 来恢复。（不持久化即从0开始apply到恢复的commitIndex）</li>
</ul>
<blockquote>
<p>对于为什么这两个状态在Raft论文中被定义为非持久化的：</p>
<p>CommitIndex 能够很快的被传播、恢复，这不是一个问题。</p>
<p>ApplyIndex 和状态机有关，你的状态机如果是持久化的，你也需要持久化 ApplyIndex 来保存这个状态。</p>
<p>当然，实际工程中对 StateMachine 的假定一般都是 Non-Volatile 的，比如 RocksDB 什么的。这样的设计下状态机不仅能够自恢复，而且自恢复往往更高效（往往只需要重放少数 WAL 即可），此时便可以考虑持久化 commitIndex 和 applyIndex 了。其实持久化 commitIndex 还好，但持久化 applyIndex 会比较麻烦，因为要想保证 safety 就需要保证状态机的更新和 applyIndex 的更新能够做到原子性，否则重启时可能会出现日志多 apply 或者少 apply 的现象。在 TiKV 的实现中，状态机的更新和 applyIndex 的更新是打包成了一条事务去做处理的，这样才算是比较优雅的解决了这个问题。</p>
</blockquote>
<p>Leader 的非持久化状态：</p>
<ul>
<li><p><code>nextIndex[]</code>：保存对于每个Follower应该发送的下一份 <code>entry index</code>；初始化为本地 lastIndex + 1，如果发送给Follower对应不上会进行相应的回退。</p>
</li>
<li><p><code>matchIndex[]</code>保存对于每个Follower已确认的Log Index，即已经同步到每一个 follower 的 <code>entry index</code>。初始化为 0，根据复制状态不断递增。</p>
<blockquote>
<p>每次选举后，Leader 的此两个数组都应该立刻重新初始化并开始构建。</p>
</blockquote>
</li>
</ul>
<h4 id="节点选举"><a href="#节点选举" class="headerlink" title="节点选举"></a>节点选举</h4><p>在<strong>节点的状态机</strong>部分已经基本介绍了什么时候会开始进行选举。这里主要介绍这个子问题中什么情况可以投赞成票选举节点成为Leader。</p>
<p><strong>① 安全选举限制</strong>（不能只比CandidateTerm哟）</p>
<p>Leader必须得确保拥有所有已commit的日志。当 Candidate 发送 RequestVoteRPC 时，会带上最后一个 log entry 的信息。 所有的节点收到该请求后，都会比对自己的日志，如果发现自己的日志更新一些，则会拒绝投票给该 Candidate。（Pre-Vote 同理，如果 follower 认为 Pre-Candidate 没有资格的话，会拒绝 PreVote）</p>
<blockquote>
<p>针对分区后部分节点会因为永远当不上Leader疯狂递增Term任期号，如果网络恢复后进入集群会更新整个集群的Term，并强行让Leader让出位置。Raft通过Pre-vote机制避免此问题，一个成员可以给所有人发探测包，如果能收到至少多数派的回复，就可以确认自己是否被网络分区，如果自己与多数派连通，那么可以发起选举。即一个 Candidate 必须在获得了多数赞同的情形下， 才会增加自己的 term，当然了Pre-Vote并不会改变其他节点的状态（Term、votefor等）。</p>
</blockquote>
<p>判断日志新旧的方式：获取请求的 entry 后，比对自己日志中的最后一个 entry。 首先比对 term，如果自己的 term 更大，则拒绝请求。 如果 term 一样，则比对 index，如果自己的 index 更大（说明自己的日志更长），则拒绝请求。</p>
<p><strong>但是光有这个限制并不能解决下面这个问题</strong>。</p>
<p>  <img src="/picture/Distributed/Figure8.png" srcset="/img/loading.gif" lazyload alt="Figure 8"></p>
<p>此问题代表要限定的另一个规则 <strong>② Leader不能提交之前任期的日志，只能通过提交自己任期的日志，从而间接提交之前任期的日志。</strong></p>
<p>先看图中错误的例子：</p>
<p>(a). S1 是任期 2 的 Leader，日志已经复制到了 S2。</p>
<p>(b). S1 宕机，S5 获得 S3、S4 和 S5 的选票成为 Leader，然后写了一条日志 &lt;index=2 , term=3&gt;。</p>
<p>(c). S5 刚写完就宕机了，S1 重新当选 Leader，currentTerm = 4，<strong>此刻还没有新的请求进来</strong>，S1 将 &lt;index=2 , term = 2&gt;的日志复制到了 S3，多数派达成，S1 提交了这个日志（注意，<strong>term=2 不是当前任期的日志，我们在讨论错误的情况</strong>）。然后请求进来，刚写了本地 &lt;index=3 , term=4&gt; 的日志，S1 就故障了。</p>
<p>(d1). 这时候 S5 可以通过来自 S2、S3、S4 和自己的投票，重新成为 Leader(currentTerm&gt;=5)，并将 index=2 &amp;&amp; term=3 的日志复制到其他所有节点并提交，此时 <strong>index=2 的日志提交了两次！</strong>一次 term=2，一次term=3，这是绝对不允许发生的，已经提交的日志不能够被覆盖！</p>
<p>(d2). 这里的情况是，S1 在宕机之前将自己 term=4 的日志复制到了大多数机器上，这样 S5 就不可能选举成功。这是 S1 不发生故障，正确复制的情况。</p>
<p>所以，<strong>我们要增加提交的约束，不让 (d1) 这种情况发生</strong>。这个约束就是，<strong>Leader 只能提交自己任期的日志</strong>，不然则会出现已提交日志被覆盖的情况。</p>
<blockquote>
<p>虽然加了这个约束不会重复提交了，但如果一直没新的请求进来，&lt;index=2 , term=3&gt; 岂不是就一直不能提交？那这里不就阻塞了吗？如果这里是 kv 数据库，问题就很明显了。假设 (c) 或 (d) 中 index=2 那条日志里的 Command 是 <code>Set(&quot;k&quot;, &quot;1&quot;)</code>，S5 当选 Leader 后，客户端来查询 <code>Get(&quot;k&quot;)</code>（已经优化到读请求不写Log了），Leader 查到日志有记录但又不能回复 <code>1</code> 给客户端（因为按照约束这条日志未提交），线性一致性要求不能返回陈旧的数据，Leader 迫切地需要知道这条日志到底能不能提交。</p>
<p>所以 raft 论文提到了引入 no-op 日志来解决这个问题。这个在 etcd 中有实现。</p>
<p><strong>no-op</strong> 日志即只有 index 和 term 信息，command 信息为空。也是要写到磁盘存储的。</p>
<p>具体流程是在 Leader 刚选举成功的时候，立即追加一条 no-op 日志，并立即复制到其它节点，no-op 日志一经提交，Leader 前面那些未提交的日志全部间接提交，问题就解决了。像上面的 kv 数据库，有了 no-op 日志之后，Leader 就能快速响应客户端查询了。</p>
<p>本质上，no-op 日志使 Leader 隐式地快速提交之前任期未提交的日志，确认当前 <code>commitIndex</code>，这样系统才会快速对外正常工作。</p>
<blockquote>
<p>同时no-op还可以解决配置变更带来的可能的数据丢失问题 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/359206808">详见此文章</a></p>
</blockquote>
</blockquote>
<p>最后对于节点的选举再说明一点，为了防止在同一时间有太多的 Follower 转变为 Candidate 导致无法选出绝对多数， Raft 采用了随机选举超时（<code>randomized election timeouts</code>）的机制， 即每个节点设定超市计时器的超时时间有一定添加一定随机值，可以有效避免同时选举的活锁问题。</p>
<h4 id="日志同步"><a href="#日志同步" class="headerlink" title="日志同步"></a>日志同步</h4><p>Leader 被选举后，则负责所有的客户端请求。每一个客户端请求都包含一个命令，该命令可以被作用到状态机上。</p>
<p>Leader 收到客户端请求后，会生成一个log entry，包含 <code>&lt;Index, Term, cmd&gt;</code>，再将这个log entry 添加到自己的日志末尾后，向所有的节点广播该log entry。</p>
<p>Follower 如果同意接受该 log entry，则在将 log entry 添加到自己的日志后，返回同意。</p>
<p>如果 Leader 收到了多数的成功答复，则将该 log entry 应用到自己的状态机上， 之后可以称该log entry 是 committed 的。该 committed 信息会随着随后的 AppendEntries 或 Heartbeat RPC 被传达到其他节点。</p>
<p><strong>不一致的Follower节点会一步步与Leader节点变为一致</strong>，在这里Raft有两点保证：</p>
<ul>
<li>如果在两个日志（节点）里，有两个 entry 拥有相同的 index 和 term，那么它们一定有相同的 cmd；</li>
<li>如果在两个日志（节点）里，有两个 entry 拥有相同的 index 和 term，那么它们前面的 entry 也一定相同。</li>
</ul>
<p>通过“仅有 Leader 可以生成 entry”来确保第一个性质， 第二个性质则通过<strong>一致性检查</strong>（consistency check）来保证，该检查包含几个步骤：</p>
<p>Leader在同步日志的RPC中还得包含真正需要发送的日志的上一条entry，Follower会进行对比自己的日志，如果发现不符则会拒绝这次AppendEntries RPC，Leader普通做法是对于该Follower的nextIndex进行减一再进行重试，对其的优化的<strong>nextIndex快速回退方法</strong>是Follower会多返回两个参数<code>ConflictIndex &amp; ConflictTerm</code>：</p>
<p>ConflictTerm = log[prevLogIndex].Term，并且查找其log中term等于 conflictTerm的第一个log的下标设置为ConflictIndex。</p>
<p>Leader接收到回复分为三种情况：</p>
<ol>
<li><p>若Leader中无这个ConflictTerm，此情形，Follower的整个ConflictTerm均得删除。所以，我们可以设置nextIndex为此Follower的ConflictTerm的第一个index即ConflictIndex。</p>
</li>
<li><p>若Leader中有这个冲突Term，此情形，Follower的这个ConflictTerm中不一定所有的index都冲突<br>所以，我们可以设置nextIndex为Leader log中ConfictTerm最后出现的位置的后一个Index。（Index + Term可以唯一确定cmd！！）</p>
</li>
<li><p>这是Follower都没有prevLogIndex的场景，这里ConflictTerm会返回-1， Leader应该回退到Follower最后一条Log条目的下一条。</p>
</li>
</ol>
<h4 id="⚠️Safety安全性"><a href="#⚠️Safety安全性" class="headerlink" title="⚠️Safety安全性"></a>⚠️Safety安全性</h4><p>对于Raft算法的安全性（各状态机状态一致性保证）有五条公理：</p>
<ol>
<li><p>选举安全特性：对于一个给定任期号，最多只有一个Leader（因为一个任期每个节点只有一张票，且过半数票才能当选Leader）。</p>
</li>
<li><p>领导人只附加原则：领导人绝不会删除或覆盖自己的日志，只会增加。</p>
</li>
<li><p>日志匹配原则：如果两个节点日志在相同index上term相同，则0～index的日志完全相同。</p>
</li>
</ol>
<blockquote>
<p>由<code>1</code>和<code>2</code>可以推出<code>3</code>。因为集群在任一时刻只有一个Leader，所以一个任期内只会在同一个index上写入一次日志。又因为Leader不会覆盖和删除自己的日志，日志一旦写入就不允许更改，所以只要term和index相同，那么在任何节点上的日志也都相同。因为跟随者每次都会根据Leader发来的PreLogIndex和PrevLogTerm寻找日志共识点，所以根据递归性质0～index的所有日志都匹配。</p>
</blockquote>
<ol start="4">
<li><p>领导人完全特性：如果某个日志条目在某个任期号中已经被提交，那么之后的新Leader必然拥有此日志（具体实现见节点选取章节的两个限制）</p>
</li>
<li><p>状态机安全特性：如果一个Leader已经将给定index的日志条目apply到了状态机上，那么其他任何一个节点不会在这个index apply一个不同的日志。</p>
</li>
</ol>
<blockquote>
<p>定义A为上个任期最后一条已提交日志</p>
<p>根据<code>4</code>，A必然同步到了集群中半数以上的节点并且Leader必然拥有A</p>
<p>根据<code>3</code>，如果A被Leader包含则比A旧的日志一定被Leader包含。</p>
<p>因为lastApplied &lt;= commitIndex 且已提交的日志在所有集群节点上顺序一致。所以apply日志必然在所有节点上顺序一致，因为状态机只能按序应用日志。所以<strong>状态机在整个集群所有节点上必然最终一致</strong>。</p>
</blockquote>
<h4 id="配置变更"><a href="#配置变更" class="headerlink" title="配置变更"></a>配置变更</h4><p>Raft 的配置变更一般分为两种方式：一次变更一个和一次变更多个。</p>
<h5 id="一次变更一台"><a href="#一次变更一台" class="headerlink" title="一次变更一台"></a>一次变更一台</h5><p>因为在 Raft 算法中，集群中每一个节点都存有整个集群的信息，而集群的成员有可能会发生变更（节点增删、替换节点等）。 Raft 限制一次性只能增／删一个节点，在一次变更结束后，才能继续进行下一次变更。</p>
<p>如果一次性只变更一个节点，那么只需要简单的要求“<strong>在新／旧集群中，都必须取得多数（N/2+1）</strong>”， 那么这两个多数中必然会出现交集，这样就可以保证不会因为配置不一致而导致脑裂。</p>
<p>  <img src="/picture/Distributed/one_config_change.png" srcset="/img/loading.gif" lazyload alt="one config change"></p>
<p>当 Leader 收到集群变更的请求后，就会生成一个特殊的 entry 项用来保存配置， 在将配置项添加到 log 后，该配置立刻生效（也就是说任何节点在收到新配置后，就立刻启用新配置）。 然后 Leader 将该 entry 扩散至多数节点，成功后则提交该 entry。 一旦一个新配置项被 committed，则视为该次变更已结束，可以继续处理下一次变更了。</p>
<p>为了保证可用性，需要新增一项规则，节点在响应 RPC 时，不考虑来源节点是否在自己的配置文件之中。 也就是说，即使收到了一个并不在自己配置文件之中的节点发来的 RPC， 也需要正常处理和响应，包括 AppendEntriesRPC 和 RequestVoteRPC。</p>
<h5 id="一次变更多台"><a href="#一次变更多台" class="headerlink" title="一次变更多台"></a>一次变更多台</h5><blockquote>
<p>Todo…</p>
</blockquote>
<h4 id="日志压缩——快照"><a href="#日志压缩——快照" class="headerlink" title="日志压缩——快照"></a>日志压缩——快照</h4><p>Raft 的日志在正常运行期间会增长以合并更多的客户请求，但是在实际的系统中，Raft 的日志无法不受限制地增长。随着日志的增长，日志会占用更多空间，并且需要花费更多时间进行重放。如果没有某种机制可以丢弃日志中累积的过时信息，这最终将导致可用性问题。因此需要定时去做 snapshot。</p>
<p>  <img src="/picture/Distributed/snapshot.png" srcset="/img/loading.gif" lazyload alt="snapshot"></p>
<p>snapshot 会包括：</p>
<ul>
<li>状态机当前的状态。</li>
<li>状态机最后一条应用的 entry 对应的 index 和 term。</li>
<li>集群最新配置信息。</li>
<li>为了保证 exactly-once 线性化语义的去重表</li>
</ul>
<p>各个节点自行择机完成自己的 snapshot 即可，如果 leader 发现需要发给某一个 follower 的 nextIndex 已经被做成了 snapshot，则需要将 snapshot 发送给该 follower。注意 follower 拿到非过期的 snapshot 之后直接覆盖本地所有状态即可，不需要留有部分 entry，也不会出现 snapshot 之后还存在有效的 entry。因此 follower 只需要判断 <code>InstallSnapshot RPC</code> 是否过期即可。过期则直接丢弃，否则直接替换全部状态即可。 </p>
<p>snapshot 可能会带来两个问题：</p>
<ol>
<li>做 snapshot 的策略？<br>一般为定时或者定大小，达到阈值即做 snapshot，做完后对状态机和 raft log 进行原子性替换即可。</li>
<li>做 snapshot 时是否还可继续提供写请求？<br>一般情况下，做 snapshot 期间需要保证状态机不发生变化，也就是需要保证 snapshot 期间状态机不处理写请求。当然 raft 层依然可以去同步，只是状态机不能变化，即不能 apply 新提交的日志到状态机中而已。要想做的更好，可以对状态机采用 <code>copy-on-write</code> 的复制来不阻塞写请求。</li>
</ol>
<h3 id="Raft优化"><a href="#Raft优化" class="headerlink" title="Raft优化"></a>Raft优化</h3><p>先回顾一下简单的Raft流程</p>
<ol>
<li>Leader 收到 client 发送的 request。</li>
<li>Leader 将 request append 到自己的 log。</li>
<li>Leader 将对应的 log entry 发送给其他的 follower。</li>
<li>Leader 等待 follower 的结果，如果大多数节点提交了这个 log，则 apply。</li>
<li>Leader 将结果返回给 client。</li>
<li>Leader 继续处理下一次 request。</li>
</ol>
<p>可以看到，上面的流程是一个典型的顺序操作，如果真的按照这样的方式来写，那性能是完全不行的。</p>
<h4 id="读写分离——读优化"><a href="#读写分离——读优化" class="headerlink" title="读写分离——读优化"></a>读写分离——读优化</h4><p>Raft对外保证线性一致性。上述简单步骤所有读写请求都通过Leader来进行，并且读请求也会按照Log进行处理。因为 Raft 本来就是一个为了实现分布式环境下线性一致性的算法，所以通过 Raft 非常方便的实现线性 Read，也就是将任何的读请求走一次 Raft Log，等此 Log 提交之后在 apply 的时候从状态机里面读取值，一定能够保证这个读取到的值是满足线性要求的。</p>
<p>当然，因为每次 Read 都需要走 Raft 流程，Raft Log 存储、复制带来刷盘开销、存储开销、网络开销，走 Raft Log不仅仅有日志落盘的开销，还有日志复制的网络开销，另外还有一堆的 Raft “读日志” 造成的磁盘占用开销，导致 Read 操作性能是非常低效的，所以在读操作很多的场景下对性能影响很大，在读比重很大的系统中是无法被接受的，通常都不会使用。</p>
<p>所以可以考虑<strong>Read不按Log处理</strong>，因为<strong>读Leader其实可以完全确保读到最新的写入数据</strong>，所以只需要确保Leader处理读请求时还是Leader即可。</p>
<h5 id="ReadIndex"><a href="#ReadIndex" class="headerlink" title="ReadIndex"></a>ReadIndex</h5><p>当 Leader 需要处理 Read 请求时，Leader 与过半机器交换心跳信息确定自己仍然是 Leader 后可提供线性一致读：</p>
<ol>
<li>Leader 将自己当前 Log 的 commitIndex 记录到一个 Local 变量 ReadIndex 里面；</li>
<li>接着向 Followers 节点发起一轮 Heartbeat，如果半数以上节点返回对应的 Heartbeat Response，那么 Leader就能够确定现在自己仍然是 Leader；</li>
<li>Leader 等待自己的 StateMachine 状态机执行，至少应用到 ReadIndex 记录的 Log，直到 applyIndex 超过 ReadIndex，这样就能够安全提供 Linearizable Read，也不必管读的时刻是否 Leader 已飘走；</li>
<li>Leader 执行 Read 请求，将结果返回给 Client。</li>
</ol>
<p>使用 ReadIndex Read 提供 Follower Read 的功能，<strong>很容易在 Followers 节点上面提供线性一致读</strong>，Follower 收到 Read 请求之后：</p>
<ol>
<li>Follower 节点向 Leader 请求最新的 ReadIndex；</li>
<li>Leader 仍然走一遍之前的流程，执行上面前 3 步的过程(确定自己真的是 Leader)，并且返回 ReadIndex 给 Follower；</li>
<li>Follower 等待当前的状态机的 applyIndex 超过 ReadIndex；</li>
<li>Follower 执行 Read 请求，将结果返回给 Client。</li>
</ol>
<blockquote>
<p>ReadIndex Read 使用 Heartbeat 方式来让 Leader 确认自己是 Leader，省去 Raft Log 流程。相比较于走 Raft Log 方式，ReadIndex Read 省去磁盘的开销，能够大幅度提升吞吐量。虽然仍然会有网络开销，但是 Heartbeat 本来就很小，所以性能还是非常好的。</p>
<p>并且还能成功将<strong>读压力分散到了各个Follower上</strong>，一举两得。</p>
</blockquote>
<h5 id="Lease-Read"><a href="#Lease-Read" class="headerlink" title="Lease Read"></a>Lease Read</h5><p>虽然 ReadIndex Read 比原来的 Raft Log Read 快很多，但毕竟还是存在 Heartbeat 网络开销，所以考虑做更进一步的优化。Raft 论文里面提及一种通过 Clock + Heartbeat 的 Lease Read 优化方法，也就是 Leader 发送 Heartbeat 的时候首先记录一个时间点 Start，当系统大部分节点都回复 Heartbeat Response，由于 Raft 的选举机制，Follower 会在 Election Timeout 的时间之后才重新发生选举，下一个 Leader 选举出来的时间保证大于 Start+Election Timeout/Clock Drift Bound，所以可以认为 Leader 的 Lease 有效期可以到 Start+Election Timeout/Clock Drift Bound 时间点。Lease Read 与 ReadIndex 类似但更进一步优化，不仅节省 Log，而且省掉网络交互，大幅提升读的吞吐量并且能够显著降低延时。</p>
<p>Lease Read 基本思路是 Leader 取一个比 Election Timeout 小的租期（最好小一个数量级），在租约期内不会发生选举，确保 Leader 不会变化，所以跳过 ReadIndex 的第二步也就降低延时。由此可见 Lease Read 的正确性和时间是挂钩的，依赖本地时钟的准确性，因此虽然采用 Lease Read 做法非常高效，但是仍然面临风险问题，也就是存在预设的前提即各个服务器的 CPU Clock 的时间是准的，即使有误差，也会在一个非常小的 Bound 范围里面，时间的实现至关重要，如果时钟漂移严重，各个服务器之间 Clock 走的频率不一样，这套 Lease 机制可能出问题。</p>
<p>Lease Read 实现方式包括：</p>
<ol>
<li>定时 Heartbeat 获得多数派响应，确认 Leader 的有效性；</li>
<li>在租约有效时间内，可以认为当前 Leader 是 Raft Group 内的唯一有效 Leader，可忽略 ReadIndex 中的 Heartbeat 确认步骤(2)；</li>
<li>Leader 等待自己的状态机执行，直到 applyIndex 超过 ReadIndex，这样就能够安全的提供 Linearizable Read。</li>
</ol>
<blockquote>
<p>这个方法主要优化Leader提供的读服务。</p>
</blockquote>
<h4 id="Batch-amp-Pipeline"><a href="#Batch-amp-Pipeline" class="headerlink" title="Batch &amp; Pipeline"></a>Batch &amp; Pipeline</h4><p>使用Batch通常情况下能提升性能，写入不会只写入一个值，而是<strong>用一个Batch 缓存一批修改，然后再整个写入</strong>。 对于 Raft 来说，Leader 可以一次收集多个 requests，然后一批发送给 Follower。当然，我们也需要有一个最大发送容量来限制每次最多可以发送多少数据。</p>
<p>但只有Batch依旧要等待Follower返回才能继续后面的流程。其实这里可以做一个假定，通常Leader和Follower建立连接后就可以认为网络是互通的。Leader根据NextIndex变量指示发送给Follower的下一个发送Log的位置，所以当 Leader 给 Follower 发送了一批Log 之后，可以像Pipeline一样<strong>直接更新 NextIndex</strong>，并且立刻发送后面的 Log，不需要等待 Follower 的返回。如果网络出现了错误，或者 Follower 返回一些错误，Leader 就需要重新调整 NextIndex，然后重新发送 Log 了。</p>
<h4 id="并行添加日志"><a href="#并行添加日志" class="headerlink" title="并行添加日志"></a>并行添加日志</h4><p>Leader 可以先<strong>并行的将 log 发送给 Followers，然后再本地log append</strong>。为什么可以这么做，主要是因为在 Raft 里面，如果一个 log 被大多数的节点append（并回复Leader），我们就可以认为这个 log 是被 committed 了，所以即使 Leader 再给 Follower 发送 log 之后，自己 append log 失败 panic 了，只要 N / 2 + 1个 Follower 能接收到这个 log 并成功 append，我们仍然可以认为这个 log 是被 committed 了，被 committed 的 log 后续就一定能被成功 apply。</p>
<p>那为什么我们要这么做呢？主要是因为 append log 会涉及到落盘，有开销，所以我们完全可以在 Leader 落盘的同时让 Follower 也尽快的收到 log 并 append。</p>
<p>这里我们还需要注意，虽然 Leader 能在 append log 之前给 Follower 发 log，但是 Follower 却不能在 append log 之前告诉 Leader 已经成功 append 这个 log。如果 Follower 提前告诉 Leader 说已经成功 append，但实际后面 append log 的时候失败了，Leader 仍然会认为这个 log 是被 committed 了，这样系统就有丢失数据的风险了。 </p>
<h4 id="异步Apply"><a href="#异步Apply" class="headerlink" title="异步Apply"></a>异步Apply</h4><p>当一个log被大部分节点append并回复Leader收到commitIndex的更新后，我们就可以认为这个log被committed了，被 committed 的 log 在什么时候被 apply 都不会再影响数据的一致性。所以<strong>当一个 log 被 committed 之后，我们可以用另一个线程去异步的 apply 这个 log</strong>。</p>
<p>所以整个 Raft 流程就可以变成：</p>
<ol>
<li>Leader 接受一个 client 发送的 request。</li>
<li>Leader 将对应的 log 发送给其他 follower 并本地 append。</li>
<li>Leader 继续接受其他 client 的 requests，持续进行步骤 2。</li>
<li>Leader 发现 log 已经被 committed，在另一个线程 apply。</li>
<li>Leader 异步 apply log 之后，返回结果给对应的 client。</li>
</ol>
<p>使用 asychronous apply 的好处在于我们现在可以完全的并行处理 append log 和 apply log，虽然对于一个 client 来说，它的一次 request 仍然要走完完整的 Raft 流程，但对于多个 clients 来说，整体的并发和吞吐量是上去了。</p>
<h4 id="…-…"><a href="#…-…" class="headerlink" title="… …"></a><strong>… …</strong></h4><h3 id="Raft与客户端的交互"><a href="#Raft与客户端的交互" class="headerlink" title="Raft与客户端的交互"></a>Raft与客户端的交互</h3><p>虽然说 raft 算法只是一个 RSM，其只需要保证不同节点上的日志相同即可，其他的事情它都不需要关心。但是要想保证线性一致性语义，对于基于 raft 的 KV 往往还需要额外做一些事情，比如即使客户端会超时重试，也要保证日志的 exactly-once 执行。这也是Lab3需要解决的问题。</p>
<blockquote>
<p>即使写请求的业务语义能够保证幂等，但不进行额外的处理让其重复执行多次也会破坏线性一致性（毕竟会覆盖其他客户端的修改）。当然，读请求由于不改变系统的状态，重复执行多次是没问题的。</p>
</blockquote>
<p>对于这个问题，raft 作者介绍了想要实现线性化语义，就需要保证日志仅被执行一次，即它可以被 commit 多次，但一定只能 apply 一次。</p>
<p>基本思路便是：</p>
<ul>
<li>每个 client 都需要一个唯一的标识符，它的每个不同命令需要有一个顺序递增的 commandId，clientId 和这个 commandId，clientId 可以唯一确定一个不同的命令，从而使得各个 raft 节点可以记录保存各命令是否已应用以及应用以后的结果。</li>
</ul>
<p>为什么要记录应用的结果？因为通过这种方式同一个命令的多次 apply 最终只会实际应用到状态机上一次，之后相同命令 apply 的时候实际上是不应用到状态机上的而是直接返回的，那么这时候应该返回什么呢？直接返回成功吗？不行，如果第一次应用时状态机报了什么例如 key not exist 等业务上的错而没有被记录，之后就很难捕捉到这个执行结果了，所以也需要将应用结果保存下来。</p>
<p>如果默认一个客户端只能串行执行请求的话，服务端这边只需要记录一个 map，其 key 是 clientId，其 value 是该 clientId 执行的最后一条日志的 commandId 和状态机的输出即可。</p>
<p>raft 论文中还考虑了对这个 map 进行一定大小的限制，防止其无线增长。这就带来了两个问题：</p>
<ul>
<li>集群间的不同节点如何就某个 clientId 过期达成共识。</li>
<li>不小心驱逐了活跃的 clientId 怎么办，其之后不论是新建一个 clientId 还是复用之前的 clientId 都可能导致命令的重执行。</li>
</ul>
<p>这些问题在工程实现上都较为麻烦。比如后者如果业务上是事务那直接 abort 就行，但如果不是事务就很难办了。</p>
<blockquote>
<p>注：配置变更及日志压缩部分暂未仔细好好研究 Todo。</p>
</blockquote>
<blockquote>
<p>其实Raft还有一些容错性问题的细节本文没有提到，那些细小的场景有可能影响Raft的正确性，也应做相应处理，具体可以详见Raft论文。</p>
</blockquote>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ol>
<li><a target="_blank" rel="noopener" href="https://tanxinyu.work/raft/">潭新宇的博客</a></li>
<li><a target="_blank" rel="noopener" href="https://hardcore.feishu.cn/docs/doccnMRVFcMWn1zsEYBrbsDf8De">硬核课堂Raft论文导读</a></li>
<li><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/382888510/answer/2368632206">潭新宇知乎回答</a></li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/jzx05Q781ytMXrZ2wrm2Vg">Raft 的 Figure 8 讲了什么问题？为什么需要 no-op 日志？</a></li>
<li><a target="_blank" rel="noopener" href="https://www.sofastack.tech/blog/sofa-jraft-linear-consistent-read-implementation/">线性一致读SOFAJRaft</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/25735592">TiKV Raft优化</a></li>
</ol>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F/">分布式</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2022/02/20/CSAPP%E9%83%A8%E5%88%86Lab%E6%80%BB%E7%BB%93/">
                        <span class="hidden-mobile">CSAPP Malloc Lab & Shell Lab</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  

  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  <script  src="/js/local-search.js" ></script>



  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  
    <script  src="https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4/anchor.min.js" ></script>
  
  
    <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" ></script>
  






  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
        typing(title);
      
    })(window, document);
  </script>












  

  

  

  

  

  





<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


</body>
</html>
